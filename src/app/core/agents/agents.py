"""Agent implementations for the multi-agent RAG flow.

This module defines four LangChain agents (Planning, Retrieval, Summarization,
Verification) and thin node functions that LangGraph uses to invoke them.
"""

from typing import List

from langchain.agents import create_agent
from langchain_core.messages import AIMessage, HumanMessage, ToolMessage

from ..llm.factory import create_chat_model
from .prompts import (
    PLANNING_SYSTEM_PROMPT,
    RETRIEVAL_SYSTEM_PROMPT,
    SUMMARIZATION_SYSTEM_PROMPT,
    VERIFICATION_SYSTEM_PROMPT,
)
from .state import QAState
from .tools import retrieval_tool


def _extract_last_ai_content(messages: List[object]) -> str:
    """Extract the content of the last AIMessage in a messages list."""
    for msg in reversed(messages):
        if isinstance(msg, AIMessage):
            return str(msg.content)
    return ""


# Internal private getters for lazy initialization
_planning_agent = None
_retrieval_agent = None
_summarization_agent = None
_verification_agent = None

def get_planning_agent():
    global _planning_agent
    if _planning_agent is None:
        _planning_agent = create_agent(
            model=create_chat_model(),
            tools=[],
            system_prompt=PLANNING_SYSTEM_PROMPT,
        )
    return _planning_agent

def get_retrieval_agent():
    global _retrieval_agent
    if _retrieval_agent is None:
        _retrieval_agent = create_agent(
            model=create_chat_model(),
            tools=[retrieval_tool],
            system_prompt=RETRIEVAL_SYSTEM_PROMPT,
        )
    return _retrieval_agent

def get_summarization_agent():
    global _summarization_agent
    if _summarization_agent is None:
        _summarization_agent = create_agent(
            model=create_chat_model(),
            tools=[],
            system_prompt=SUMMARIZATION_SYSTEM_PROMPT,
        )
    return _summarization_agent

def get_verification_agent():
    global _verification_agent
    if _verification_agent is None:
        _verification_agent = create_agent(
            model=create_chat_model(),
            tools=[],
            system_prompt=VERIFICATION_SYSTEM_PROMPT,
        )
    return _verification_agent


def _parse_planning_output(content: str):
    """Parse the structured output from the Planning Agent."""
    plan = ""
    sub_questions = []

    lines = content.strip().split("\n")
    current_section = None

    for line in lines:
        if line.startswith("Plan:"):
            plan = line.replace("Plan:", "").strip()
        elif line.startswith("Sub-questions:"):
            current_section = "sub_questions"
        elif current_section == "sub_questions" and line.strip().startswith("-"):
            sub_questions.append(line.strip().lstrip("- ").strip())

    return plan, sub_questions


def planning_node(state: QAState) -> QAState:
    """Planning Agent node: decomposes question into a search strategy.

    This node:
    - Sends the user's question to the Planning Agent.
    - Extracts the natural language 'plan' and list of 'sub_questions'.
    - Stores them in the state for the Retrieval Agent to use.
    """
    question = state["question"]

    agent = get_planning_agent()
    result = agent.invoke({"messages": [HumanMessage(content=question)]})
    content = _extract_last_ai_content(result.get("messages", []))

    plan, sub_questions = _parse_planning_output(content)

    # Fallback: if no sub-questions parsed, use the original question
    if not sub_questions:
        sub_questions = [question]

    return {
        "plan": plan,
        "sub_questions": sub_questions,
    }


def retrieval_node(state: QAState) -> QAState:
    """Retrieval Agent node: gathers context from vector store.

    This node:
    - Iterates through the sub-questions generated by the Planning Agent.
    - Sends each sub-question to the Retrieval Agent.
    - Consolidates all retrieved context from all sub-queries.
    - Stores the unique context string in `state["context"]`.
    """
    sub_questions = state.get("sub_questions") or [state["question"]]
    all_contexts = []

    agent = get_retrieval_agent()
    for sq in sub_questions:
        result = agent.invoke({"messages": [HumanMessage(content=sq)]})
        messages = result.get("messages", [])

        # Extract context from ToolMessage
        for msg in reversed(messages):
            if isinstance(msg, ToolMessage):
                all_contexts.append(str(msg.content))
                break

    # Join multiple retrieval results
    context = "\n\n".join(all_contexts)

    return {
        "context": context,
    }


def summarization_node(state: QAState) -> QAState:
    """Summarization Agent node: generates draft answer from context.

    This node:
    - Sends question + context to the Summarization Agent.
    - Agent responds with a draft answer grounded only in the context.
    - Stores the draft answer in `state["draft_answer"]`.
    """
    question = state["question"]
    context = state.get("context")

    user_content = f"Question: {question}\n\nContext:\n{context}"

    agent = get_summarization_agent()
    result = agent.invoke(
        {"messages": [HumanMessage(content=user_content)]}
    )
    messages = result.get("messages", [])
    draft_answer = _extract_last_ai_content(messages)

    return {
        "draft_answer": draft_answer,
    }


def verification_node(state: QAState) -> QAState:
    """Verification Agent node: verifies and corrects the draft answer.

    This node:
    - Sends question + context + draft_answer to the Verification Agent.
    - Agent checks for hallucinations and unsupported claims.
    - Stores the final verified answer in `state["answer"]`.
    """
    question = state["question"]
    context = state.get("context", "")
    draft_answer = state.get("draft_answer", "")

    user_content = f"""Question: {question}

Context:
{context}

Draft Answer:
{draft_answer}

Please verify and correct the draft answer, removing any unsupported claims."""

    agent = get_verification_agent()
    result = agent.invoke(
        {"messages": [HumanMessage(content=user_content)]}
    )
    messages = result.get("messages", [])
    answer = _extract_last_ai_content(messages)

    return {
        "answer": answer,
    }
